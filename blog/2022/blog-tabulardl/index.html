<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> What are the Effective Deep Learning Models for Tabular Data? | Bowen Fang </title> <meta name="author" content="Bowen Fang"> <meta name="description" content="The personal academic website of Bowen Fang. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bwfbowen.github.io/blog/2022/blog-tabulardl/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Bowen Fang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">blog</a> <a class="dropdown-item " href="/publications/">publications</a> <a class="dropdown-item " href="/cv/">cv</a> <a class="dropdown-item " href="/projects/">projects</a> <a class="dropdown-item " href="/repositories/">repositories</a> <a class="dropdown-item " href="/teaching/">teaching</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">What are the Effective Deep Learning Models for Tabular Data? </h1> <p class="post-meta"> Created on March 13, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/dl"> <i class="fa-solid fa-hashtag fa-sm"></i> DL</a>   <a href="/blog/tag/tabular-data"> <i class="fa-solid fa-hashtag fa-sm"></i> Tabular Data</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> Pytorch</a>   <a href="/blog/tag/transformer"> <i class="fa-solid fa-hashtag fa-sm"></i> Transformer</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This week, I would like to share a paper published at NeurIPS 2021. When dealing with tabular data, I often find myself perplexed. On one hand, I am unsure which deep learning frameworks are better suited for this task, and on the other hand, I am uncertain whether the time-consuming process of training a model can outperform the easily accessible GBDT family of models such as XGBoost and LightGBM. However, this paper provides a detailed and comprehensive comparison of deep learning algorithms and GBDT models on tabular data. It introduces new baselines and presents a novel architecture that outperforms other deep learning models. I have gained a lot from this paper and would like to share it with you.</p> <blockquote> <p>Original Paper Information <br> <strong>Title</strong>: Revisiting Deep Learning Models for Tabular Data <br> <strong>Author</strong>: Yury Gorishniy, Ivan Rubachev, Valentin Khrulkov, Artem Babenko <br> <strong>Code</strong>: <a href="https://github.com/yandex-research/rtdl" rel="external nofollow noopener" target="_blank">https://github.com/yandex-research/rtdl</a></p> </blockquote> <h1 id="background">Background</h1> <p>Deep learning has achieved significant success in the domains of image, audio, and text data, which has sparked interest in applying deep learning to tabular data. Tabular data refers to data points represented as vectors with diverse features, stored in a tabular form. Such data is commonly encountered in industrial applications and machine learning competitions.</p> <p>However, despite the proliferation of deep learning models applied to tabular data, previous studies lacked sufficient benchmarks and suffered from issues such as insufficient comparisons and inconsistent datasets. Additionally, while there have been novel architectures proposed in this field, there is still a lack of a simple, reliable, and competitive baseline. MLP remains the main baseline in this field, but it falls short in terms of competitiveness, despite its simplicity.</p> <p>Therefore, the authors compared mainstream deep learning models on various commonly used datasets with the same training framework. The authors also proposed two simple yet competitive frameworks: ResNet-like MLP, which is easy to tune but is outperformed by other models across multiple datasets, and FT-Transformer, which is a simple modification of the Transformer architecture and exhibits superior performance on the majority of datasets. Finally, the paper compared several state-of-the-art deep learning models with GBDT, concluding that neither approach is globally superior, and both have their strengths and weaknesses.</p> <h1 id="models-to-compare">Models to compare</h1> <p>\(\newcommand{\mlp}{\mathrm{MLP}} \newcommand{\mlpb}{\mathrm{MLPBlock}} \newcommand{\lin}{\mathrm{Linear}} \newcommand{\drop}{\mathrm{Dropout}} \newcommand{\relu}{\mathrm{ReLU}} \newcommand{\resn}{\mathrm{ResNet}} \newcommand{\resb}{\mathrm{ResNetBlock}} \newcommand{\pred}{\mathrm{Prediction}} \newcommand{\bn}{\mathrm{BatchNorm}} \newcommand{\stack}{\mathrm{stack}} \newcommand{\fttrans}{\mathrm{FT-Transformer}} \newcommand{\fttb}{\mathrm{Block}} \newcommand{\ft}{\mathrm{FeatureTokenizer}} \newcommand{\ffn}{\mathrm{FFN}} \newcommand{\rpn}{\mathrm{ResidualPreNorm}} \newcommand{\norm}{\mathrm{Norm}} \newcommand{\act}{\mathrm{Activation}} \newcommand{\module}{\mathrm{Module}} \newcommand{\mhsa}{\mathrm{MHSA}} \newcommand{\acls}{\mathrm{AppendCLS}} \newcommand{\layern}{\mathrm{LayerNorm}}\) This section describes the models used for comparison. There are some symbols and concepts that need to be clarified:</p> <p>The paper focuses on supervised learning problems. $D={(x_i,y_i)}$ represents the dataset, where $x_i=(x_i^{num},x_i^{cat})$ represents the numerical and categorical features respectively, and $yi$ represents the corresponding labels. There are a total of $k$ features. The dataset is divided into three disjoint subsets: $D=D_{train} \cup D_{val} \cup D_{test}$. $D_{train}$ is used for model training, $D_{val}$ is used for hyperparameter tuning and early stopping, and $D_{test}$ is used for final evaluation.</p> <p>The tasks encompass three types: binary classification, regression, and multi-class classification.</p> <h2 id="1-mlp">1. MLP</h2> <p>Each Multilayer Perceptron (MLP) block consists of three parts:</p> <ol> <li>One linear layer;</li> <li>ReLU activation;</li> <li>Dropout layer.</li> </ol> <p>Multiple MLP blocks are nested together, and the output is passed through a final linear layer. \(\mlp(x)=\lin(\mlpb(...(\mlpb(x))))\)</p> <h2 id="2-resnet-like-mlp">2. ResNet-like MLP</h2> <p>The paper introduces a simple variant of ResNet, and the structure of the ResNet model in the paper is as follows: \(\begin{align} \resn(x) &amp; =\pred(\resb(...(\resb(\lin(x))))) \\ \resb(x) &amp; =x+\drop(\lin(\drop(\relu(\lin(\bn(x)))))) \\ \pred(x) &amp; =\lin(\relu(\bn(x))) \end{align}\) It consists of multiple nested residual blocks, followed by batch normalization, ReLU activation, and a linear layer for output. Each residual block includes batch normalization, a linear layer, ReLU activation, Dropout, another linear layer, Dropout, and a residual connection.</p> <h2 id="3-ft-transformer">3. FT-Transformer</h2> <p>The authors proposed the FT-Transformer (Feature Tokenizer Transformer) architecture, as shown below. It consists of two parts: the first part involves mapping all the features to embedding vectors, while the second part applies a series of Transformer blocks to these vectors.</p> <figure> <img src="/assets/img/fttrans.png" alt="ft-transformer"> <figcaption style="text-align: center">Figure 1. In the FT-Transformer architecture, the first step is to map all the features to vectors through an embedding process. Then, a special token, [CLS], is added to the mapped vectors to serve as the final prediction of the model. </figcaption> </figure> <p>The Feature Tokenizer block performs a linear transformation and adds bias on numerical features, with independent weights for each numerical feature. For categorical variables, each category is mapped to a distinct vector, and the categorical variables are treated independently. This process results in a $k\times d$-dimensional vector $T$. The formula can be expressed as follows: \(\begin{align} &amp; T_j^{(num)} = b_j^{(num)}+x_j \cdot W_j^{(num)} &amp; \in \mathbb{R}^d,\\ &amp; T_j^{(cat)} = b_j^{(cat)}+e_j^TW_j^{(cat)} &amp; \in\mathbb{R}^d, \\ &amp; T = \stack[T_1^{(num)},...,T_{k^{(num)}}^{(num)},T_1^{(cat)},...,T_{k^{(cat)}}^{(cat)}] &amp; \in \mathbb{R}^{k\times d}. \end{align}\)</p> <p>The Transformer block starts by adding a [CLS] token to the beginning of the vector $T$. Each block in the Transformer consists of a multi-head self-attention and a feed-forward network. Both parts of the block undergo layer normalization and residual connections. \(\begin{align} \fttrans(x)&amp;=\pred(\fttb(...(\fttb(\acls(\ft(x)))))) \\ \fttb(x)&amp;=\rpn(\ffn,\rpn(\mhsa,x)) \\ \rpn(\module, x) &amp;= x + \drop(\module(\norm(x))) \\ \ffn(x) &amp;= \lin(\drop(\act(\lin(x)))) \end{align}\) The final prediction is made using the extracted [CLS] vector after passing through multiple layers, which are layer normalization, ReLU activation, and a linear layer. \(\hat{y}=\lin(\relu(\layern(T_L^{[CLS]}))).\)</p> <figure> <img src="/assets/img/fttrans_struct.png" alt="ft-transformer"> <figcaption style="text-align: center">Figure 2. (a) is Feature Tokenizer, (b) is one Transformer block. </figcaption> </figure> <h2 id="4-other-models">4. Other models</h2> <p>In this section, the authors briefly mention several models: SNN, NODE, TabNet, GrowNet, DCN V2, AutoInt, XGBoost, and CatBoost.</p> <p>It is mentioned in the appendix that <a href="https://github.com/Qwicen/node" rel="external nofollow noopener" target="_blank">NODE</a>, <a href="https://github.com/google-research/google-research/tree/master/tabnet" rel="external nofollow noopener" target="_blank">TabNet</a>, and <a href="https://github.com/sbadirli/GrowNet" rel="external nofollow noopener" target="_blank">GrowNet</a> utilize official open-source implementations. As for the other deep learning models, they were implemented by the authors themselves, and the source code can be found in their <a href="https://github.com/yandex-research/rtdl" rel="external nofollow noopener" target="_blank">open-source repository</a>.</p> <h1 id="experiments">Experiments</h1> <h2 id="1-datasets">1. Datasets</h2> <p>Apart from Appendix, 11 publicly available datasets are compared in the paper. Each dataset undergoes a single data split, and all models are trained, validated, and tested on the exact same data to ensure fair comparison.</p> <figure> <img src="/assets/img/data_cmp_rtdl.png" alt="datasets"> <figcaption style="text-align: center">Figure 3. For the 11 datasets, the evaluation metric for regression problems is the root mean square error (RMSE), while for binary classification and multi-class classification, the evaluation metric is accuracy. The terms "num. features" and "cat. features" represent the number of numerical features and categorical features, respectively. </figcaption> </figure> <h2 id="2-implementation-details">2. Implementation details</h2> <ul> <li>Preprocessing: <ul> <li>Data preprocessing has a significant impact on the performance of deep learning models. By default, the quantile transform from <code class="language-plaintext highlighter-rouge">scikit-learn</code> is used. Normalization is applied to the <code class="language-plaintext highlighter-rouge">Helena</code> and <code class="language-plaintext highlighter-rouge">ALOI</code> datasets, while for the Epsilon dataset, it was found that preprocessing had a detrimental effect, so the original data was used. The regression targets are standardized.</li> </ul> </li> <li>Hyperparameter Tuning: <ul> <li>Hyperparameters are tuned using a validation set. The authors mention the use of the <code class="language-plaintext highlighter-rouge">Optuna</code> tool for Bayesian optimization, which has been shown to outperform random search (Turner et al., 2021). The comparisons in the main body of the paper are limited to a specific number of iterations, and comparisons with limited time are provided in the appendix.</li> </ul> </li> <li>Evaluation: <ul> <li>The experiments are run with different random seeds, and the performance on the test set is averaged over 15 runs.</li> </ul> </li> <li>Ensemble Methods: <ul> <li>Ensemble methods are considered in the experiments. For each model, the 15 individual models are divided into three disjoint groups, and the outputs of the models within each group are averaged.</li> </ul> </li> <li>Neural Networks: <ul> <li>For classification problems, cross-entropy loss is used, while for regression problems, mean squared error is used. <code class="language-plaintext highlighter-rouge">TabNet</code> and <code class="language-plaintext highlighter-rouge">GrowNet</code> follow the original papers and use the <code class="language-plaintext highlighter-rouge">Adam</code> optimizer, while the others use <code class="language-plaintext highlighter-rouge">AdamW</code>. All models terminate training if there is no improvement on the validation set for 17 consecutive epochs.</li> </ul> </li> <li>Handling Categorical Variables: <ul> <li>For <code class="language-plaintext highlighter-rouge">XGBoost</code>, one-hot encoding is used, while <code class="language-plaintext highlighter-rouge">CatBoost</code> utilizes its built-in methods for categorical variable handling. For neural networks, embeddings of the same dimensions are used, following the approach described in the <code class="language-plaintext highlighter-rouge">FT-Transformer</code>.</li> </ul> </li> </ul> <h2 id="3-result">3. Result</h2> <h3 id="31-deep-learning-models-comparison">3.1 Deep learning models comparison</h3> <p>The result is shown below:</p> <figure> <img src="/assets/img/dl_cmp_rtdl.png" alt="deep learning models comparison"> <figcaption style="text-align: center">Figure 4. Deep learning models comparison. Bold indicates the best-performing model for each task. </figcaption> </figure> <p>To summarize,</p> <ol> <li> <p>MLP remains a good sanity check.</p> </li> <li> <p>ResNet serves as an effective baseline, as no other model consistently outperforms it.</p> </li> <li> <p>Fine-tuning can make MLP and ResNet competitive, so the authors recommend tuning the parameters of the baselines when feasible. They also mention the helpfulness of Optuna in parameter tuning.</p> </li> <li> <p>Next, the authors found that the NODE model performs well on multiple tasks. However, it has a larger parameter count compared to ResNet and FT-Transformer, and it employs a framework similar to ensemble learning. Therefore, the authors further compared the performance of NODE, ResNet, and FT-Transformer using ensembling.</p> </li> </ol> <figure> <img src="/assets/img/ensemble_dl_rtdl.png" alt="Ensemble deep learning models comparison"> <figcaption style="text-align: center">Figure 5. Ensemble deep learning models comparison. Bold indicates the best-performing model for each task. </figcaption> </figure> <p>It can be observed that ResNet and FT-Transformer benefit more from ensembling, while NODE’s improvement is relatively smaller. Additionally, FT-Transformer consistently outperforms the NODE model.</p> <h3 id="32-deep-learning-vs-gbdt">3.2 Deep learning vs GBDT</h3> <p>In this section, all deep learning models are compared with GBDT models in ensemble ways. The results are as follows:</p> <figure> <img src="/assets/img/gbdt_dl_rtdl.png" alt="Ensemble deep learning models vs GBDT"> <figcaption style="text-align: center">Figure 6. Ensemble deep learning models and GBDT models comparison. Bold indicates the best-performing model for each task. </figcaption> </figure> <p>The authors compared the performance of default and tuned parameters. The performance of default parameters is important since it is a common scenario in practice. It can be seen that the ensemble of FT-Transformer with default parameters performs on par with the tuned FT-Transformer.</p> <p>After tuning the parameters, GBDT dominates in some datasets, and the difference in performance is significant. This indicates that there is no general superiority between deep learning and GBDT. However, for multi-class classification problems with a larger number of classes, GBDT is not particularly suitable. For example, in the case of Helena with 100 classes, GBDT’s performance after tuning is unsatisfactory, and for ALOI with 1000 classes, the training process is slow and it becomes challenging to tune the GBDT model.</p> <p>The default parameters of FT-Transformer is given in the Appendix:</p> <figure> <img src="/assets/img/dparam_fttrans_rtdl.png" alt="Default parameters of FT-Transformer"> <figcaption style="text-align: center">Figure 7. The default parameter of FT-Transformer. </figcaption> </figure> <h2 id="4-inspiring-question-when-is-ft-transformer-better-than-resnet">4. Inspiring question: When is FT-Transformer better than ResNet?</h2> <p>The author observed that on datasets where GBDT outperforms ResNet, FT-Transformer also exhibits a larger advantage over ResNet. On other datasets, the performance of the two models is relatively close, which the author observed in both single-model and ensemble settings.</p> <p>Therefore, the author conducted a series of synthetic tasks to demonstrate when FT-Transformer is better than ResNet, ranging from negligible performance difference to significant gaps.</p> <p>First, the author generated and fixed a series of data points ${x_i}$ and performed a single train-validate-test split. Two regression targets were defined: $f_{GBDT}$, which is expected to be simpler for GBDT, and $f_{DL}$, which is expected to be simpler for ResNet. The definitions are as follows:</p> \[x\sim \mathcal{N}(0,I_k), \\ y=\alpha f_{GBDT}(x)+(1-\alpha)f_{DL}(x).\] <p>where $f_{GBDT}(x)$ is the average output of 30 randomly generated decision trees, $f_{DL}(x)$ is an MLP with 3 randomly initialized layers. The target $y$ is normalized before training.</p> <figure> <img src="/assets/img/syn_cmp_rtdl.png" alt="FT-Transformer vs ResNet"> <figcaption style="text-align: center">Figure 8. 5 experiment results on test set, each alpha represents a task. </figcaption> </figure> <p>It can be observed that on tasks that are simpler for ResNet (small $\alpha$), both deep learning models outperform CatBoost. However, as the tasks become more GBDT-friendly (large $\alpha$), the performance of the ResNet model significantly declines. On the other hand, FT-Transformer exhibits competitive performance across all tasks.</p> <p>This experiment demonstrates that FT-Transformer is better than ResNet at fitting functions that are based on decision trees. This finding may be related to the previous observations.</p> <h2 id="5-ablation-experiments">5. Ablation Experiments</h2> <p>In this section, the author conducted several ablation experiments on the implementation choices of FT-Transformer.</p> <p>First, the author compared the AutoInt model, which also maps features to embedding vectors and utilizes self-attention. However, there are differences in the implementation details. AutoInt does not include biases during feature mapping, and its core structure differs significantly from the architecture described in Vaswani et al., 2017, the canonical Transformer. Additionally, AutoInt does not utilize techniques such as adding [CLS] during inference.</p> <p>Using the same procedure as before, the author examined the performance without adding biases, as shown in the figure:</p> <figure> <img src="/assets/img/ablation_rtdl.png" alt="Ablation Transformer"> <figcaption style="text-align: center">Figure 9. 2 attention mechanism and w/o feature biases comparison. </figcaption> </figure> <p>It can be observed that the core of Transformer is better than AutoInt, and including feature biases yields better results compared to excluding them.</p> <h2 id="conclusion">Conclusion</h2> <p>This paper provides a comprehensive comparison of mainstream deep learning models on multiple tabular datasets and improves the baseline standards for deep learning on tabular data. Firstly, it demonstrates the effectiveness of ResNet-like architectures as strong baselines. Secondly, it introduces FT-Transformer, which outperforms other deep learning models on the majority of datasets. Thirdly, through the comparison of deep learning models with GBDT, it reveals that GBDT still dominates on certain datasets.</p> <h2 id="appendix">Appendix</h2> <p>In the appendix, there is still a wealth of noteworthy information. It includes the parameter spaces for hyperparameter tuning of each model, which is not elaborated here. Instead, I will highlight some additional experiments.</p> <h3 id="training-time">Training Time</h3> <p>Firstly, the authors present a comparison of the training time between ResNet and FT-Transformer on the 11 datasets discussed:</p> <figure> <img src="/assets/img/time_cmp_rtdl.png" alt="Training time comparison"> <figcaption style="text-align: center">Figure 10. Training time comparison of FT-Transformer and ResNet-like MLP in seconds. The huge difference on Yahoo(YA) dataset might be resulted from large feature size. </figcaption> </figure> <p>Next, the authors attempted to limit the tuning time and observe the performance of different models. In this experiment, XGBoost, MLP, ResNet, and FT-Transformer were used. The experiment was conducted on three datasets: California Housing, Adult, and Higgs Small. The results are as follows:</p> <figure> <img src="/assets/img/tlimit_cmp_rtdl.png" alt="Limit training time comparison"> <figcaption style="text-align: center">Figure 11. The number of iterations for tuning with Optuna is indicated in parentheses. The red bold font represents the best performance among all models, while the black bold font represents the best performance among the deep learning models. </figcaption> </figure> <p>It can be observed that:</p> <ol> <li>FT-Transformer performs well in a few rounds of random parameter selection (within the first 10 rounds of default random selection with Optuna).</li> <li>FT-Transformer has a slower training speed compared to other models.</li> <li>Additional iterations have limited significance for other models.</li> </ol> <h3 id="other-datasets">Other datasets</h3> <p>The appendix provides the performance of different models on four datasets that were not mentioned in the main text. The author found that all models perform similarly on these datasets, but the limited information provided does not warrant their inclusion in the main text. The four datasets are as follows:</p> <figure> <img src="/assets/img/other_rtdl.png" alt="Other datasets for comparison"> <img src="/assets/img/other_cmp_rtdl.png" alt="Other datasets for comparison"> <figcaption style="text-align: center">Figure 12. Additonal datasets. </figcaption> </figure> <h2 id="review-comments">Review comments</h2> <p>From the <a href="https://openreview.net/forum?id=i_Q1yrOegLY" rel="external nofollow noopener" target="_blank">openreview</a>, we can see the questions raised by the reviewers before the paper was accepted. Some of the answers to these questions are already reflected in the appendix, such as the changes in model performance over time and the datasets like “click.” However, I would like to highlight the question regarding the selection of GBDT models.</p> <p>One comment mentioned that both XGBoost and CatBoost use level-wise trees, and it would be interesting to explore the use of leaf-wise trees like LightGBM.</p> <p>The author provided experiment results on LightGBM:</p> <figure> <img src="/assets/img/lightgbm_rtdl.png" alt="LightGBM for comparison"> <figcaption style="text-align: center">Figure 13. Comparison of LightGBM and other models on CA, AD, HI datasets. </figcaption> </figure> <p>CA and AD are datasets where GBDT performs well, and HI is a dataset where deep learning and GBDT perform similarly. For CA, lower values are better, while higher values are better for the other two. It can be observed that LightGBM performs similarly to the other two GBDT models, which aligns with the author’s expectations.</p> <p>The author mentioned that deep learning models were not able to surpass GBDT on datasets that are more favorable to GBDT. Therefore, additional GBDT models were not included. However, if deep learning models start to outperform some GBDT models on datasets that are more GBDT-friendly, it would be necessary to include more GBDT models for comparison.</p> <h1 id="implementation">Implementation</h1> <p>The article provides a comprehensive comparison and answers questions that have long puzzled me. Yandex is a Russian search giant, and CatBoost is also from this company, which is worth paying attention to.</p> <p>At the same time, I have learned two new things: FT-Transformer and Optuna. I have tried both tools personally, and I would like to share the code for each.</p> <p><code class="language-plaintext highlighter-rouge">ft_transformer.py</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">from</span> <span class="n">unicodedata</span> <span class="kn">import</span> <span class="n">name</span>
<span class="kn">from</span> <span class="n">gzip</span> <span class="kn">import</span> <span class="n">GzipFile</span>
<span class="kn">import</span> <span class="n">torch</span> 
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> 
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span><span class="p">,</span> <span class="n">quantile_transform</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span> 
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span> 
<span class="kn">import</span> <span class="n">os</span> 
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> 
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>


<span class="k">class</span> <span class="nc">PositionalEncoding</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">float</span><span class="p">()</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
        <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="nf">register_buffer</span><span class="p">(</span><span class="sh">'</span><span class="s">pe</span><span class="sh">'</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        x: [sequence length, batch size, embed dim]
        output: [sequence length, batch size, embed dim]
        </span><span class="sh">'''</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">pe</span><span class="p">[:</span><span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cats_data</span><span class="p">,</span> <span class="n">numers_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cats_data</span> <span class="o">=</span> <span class="n">cats_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">numers_data</span> <span class="o">=</span> <span class="n">numers_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">cats_data</span><span class="p">)</span>
    

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">cats_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">numers_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>


    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
       <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">length</span>         


<span class="k">class</span> <span class="nc">NumerFeatureTokenizer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">feature_dim</span><span class="p">,</span> 
        <span class="n">embedding_dim</span><span class="p">,</span>
        <span class="n">device</span>
        <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span> <span class="o">=</span> <span class="n">feature_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">feature_dim</span><span class="p">)])</span>
    

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_numer</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        x_numer: [batch size, feature dim]
        output: [batch size, feature dim, embedding dim]
        </span><span class="sh">'''</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x_numer</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span> <span class="o">==</span> <span class="n">x_numer</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="sh">'</span><span class="s">特征大小不等</span><span class="sh">'</span>
        <span class="n">x_numer</span> <span class="o">=</span> <span class="n">x_numer</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x_numer</span><span class="p">.</span><span class="n">device</span>
        <span class="n">tensor_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">tensor_embed</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x_numer</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">idx</span><span class="p">])</span>
             
        <span class="k">return</span> <span class="n">tensor_embed</span>


<span class="k">class</span> <span class="nc">CatFeatureTokenizer</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span>
        <span class="n">max_cats</span><span class="p">,</span> 
        <span class="n">embedding_dim</span><span class="p">,</span>
        <span class="n">device</span>
        <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">max_cats</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">embedding_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">([</span><span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">max_cats</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">embedding_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span><span class="p">)])</span>
    

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x_cat</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        x_cat: [batch size, feature dim]
        output: [batch size, feature dim, embedding dim]
        </span><span class="sh">'''</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x_cat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span> <span class="o">==</span> <span class="n">x_cat</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="sh">'</span><span class="s">特征大小不等</span><span class="sh">'</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">x_cat</span><span class="p">.</span><span class="n">device</span>
        <span class="n">tensor_embed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">feature_dim</span><span class="p">),</span> <span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
            <span class="n">tensor_embed</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizers</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">x_cat</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">])</span>
             
        <span class="k">return</span> <span class="n">tensor_embed</span>


<span class="k">class</span> <span class="nc">TransformerClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span>
        <span class="n">self</span><span class="p">,</span> 
        <span class="n">num_classes</span><span class="p">,</span>
        <span class="n">cat_tokenizer</span><span class="p">:</span> <span class="n">CatFeatureTokenizer</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">numer_tokenizer</span><span class="p">:</span> <span class="n">NumerFeatureTokenizer</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">nhead</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span>
        <span class="n">dim_feedforward</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
        <span class="n">dim_feedforward_size_factor</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">/</span> <span class="mi">3</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">norm</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> 
        <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">TransformerClassifier</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span> <span class="o">=</span> <span class="n">cat_tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span> <span class="o">=</span> <span class="n">numer_tokenizer</span>

        <span class="k">if</span> <span class="n">cat_tokenizer</span> <span class="ow">and</span> <span class="n">numer_tokenizer</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">==</span> <span class="n">numer_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="sh">'</span><span class="s">inconsistent tokenizer dimensions</span><span class="sh">'</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">elif</span> <span class="n">cat_tokenizer</span><span class="p">:</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">elif</span> <span class="n">numer_tokenizer</span><span class="p">:</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">numer_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">'</span><span class="s">at least one tokenizer is needed</span><span class="sh">'</span><span class="p">)</span>

        
        <span class="k">assert</span> <span class="n">d_model</span> <span class="o">%</span> <span class="n">nhead</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="sh">'</span><span class="s">d_model % nhead != 0</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">d_model</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="k">if</span> <span class="n">dim_feedforward</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">dim_feedforward</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="n">dim_feedforward_size_factor</span> <span class="o">*</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="n">encoder_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoderLayer</span><span class="p">(</span><span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="n">d_model</span><span class="p">,</span> <span class="n">nhead</span><span class="o">=</span><span class="n">nhead</span><span class="p">,</span> <span class="n">dim_feedforward</span><span class="o">=</span><span class="n">dim_feedforward</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">transformer_encoder</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">encoder_layer</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="n">norm</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        x: (x_cats, x_numer) tuple; or x_cats/ x_numer tensor
        shape: [batch_size, feature_dim, embedding_dim]
        output: [batch_size, num_classes]
        </span><span class="sh">'''</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span><span class="p">:</span>
                <span class="n">x_cats</span><span class="p">,</span> <span class="n">x_numer</span> <span class="o">=</span> <span class="n">x</span> 
                <span class="n">x_cats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cat_tokenizer</span><span class="p">(</span><span class="n">x_cats</span><span class="p">)</span>
                <span class="n">x_numer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">numer_tokenizer</span><span class="p">(</span><span class="n">x_numer</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">x_cats</span><span class="p">,</span> <span class="n">x_numer</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Tokenizer not found</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cat_tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">numer_tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span>      
        <span class="n">cls</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">d_model</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">cls</span><span class="p">,</span> <span class="n">x</span><span class="p">])</span>
        <span class="c1"># self.temp = x 
</span>        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">transformer_encoder</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="c1"># self.tmp = x 
</span>        <span class="c1"># x = x.mean(dim=1)
</span>        <span class="n">cls</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">cls</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">activation</span><span class="p">(</span><span class="n">cls</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">cls</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>


<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">:</span> <span class="n">TransformerClassifier</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">save_to</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">transformer_val_best_model</span><span class="sh">'</span><span class="p">),</span> <span class="n">is_lstm</span> <span class="o">=</span> <span class="bp">False</span><span class="p">):</span>
    <span class="n">loss_trace</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">best_val_score</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span>  <span class="o">=</span> <span class="n">x_cat_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_lstm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">))</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
        <span class="n">loss_trace</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">current_loss</span><span class="p">)</span>

        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">val_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">val_score</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">()</span>
        <span class="n">pred_val</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            
                <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span>  <span class="o">=</span> <span class="n">x_cat_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">is_lstm</span><span class="p">:</span>
                    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
                    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
                <span class="n">pred_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
                <span class="n">pred_val</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_batch</span><span class="p">)</span>
                <span class="n">val_score</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_batch</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">))</span>
                <span class="n">val_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>
            <span class="c1"># val_loss = np.mean(val_loss)
</span>        
        <span class="n">val_score</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">val_score</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">pred_val</span><span class="p">))</span>
        
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> val loss: </span><span class="si">{</span><span class="n">val_loss</span><span class="si">}</span><span class="s"> </span><span class="se">\t</span><span class="s"> train loss: </span><span class="si">{</span><span class="n">current_loss</span><span class="si">}</span><span class="s"> </span><span class="se">\n</span><span class="s"> val score: </span><span class="si">{</span><span class="n">val_score</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span> <span class="ow">or</span> <span class="n">val_score</span> <span class="o">&gt;</span> <span class="n">best_val_score</span><span class="p">:</span>
            <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sh">'</span><span class="s">epoch</span><span class="sh">'</span><span class="p">:</span> <span class="n">epoch</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">model_state_dict</span><span class="sh">'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">(),</span>
                <span class="sh">'</span><span class="s">best_val_loss</span><span class="sh">'</span><span class="p">:</span> <span class="n">val_loss</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">best_val_score</span><span class="sh">'</span><span class="p">:</span> <span class="n">val_score</span>
            <span class="p">},</span> 
            <span class="n">save_to</span>
            <span class="p">)</span>
            <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">model saved</span><span class="sh">'</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">val_loss</span> <span class="o">&lt;</span> <span class="n">best_val_loss</span><span class="p">:</span>
                <span class="n">best_val_loss</span> <span class="o">=</span> <span class="n">val_loss</span>
            <span class="k">if</span> <span class="n">val_score</span> <span class="o">&gt;</span> <span class="n">best_val_score</span><span class="p">:</span>
                <span class="n">best_val_score</span> <span class="o">=</span> <span class="n">val_score</span>

        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span>
            <span class="p">{</span>
                <span class="sh">'</span><span class="s">epoch</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">last</span><span class="sh">'</span><span class="p">,</span>
                <span class="sh">'</span><span class="s">model_state_dict</span><span class="sh">'</span><span class="p">:</span> <span class="n">model</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()</span>
            <span class="p">},</span> 
            <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">transformer_last_epoch</span><span class="sh">'</span><span class="p">)</span>
            <span class="p">)</span>
    <span class="c1"># loss curve
</span>    <span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">loss_trace</span><span class="p">,</span> <span class="sh">'</span><span class="s">r-</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Epoch</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">'</span><span class="s">Loss</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>  
        

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">is_lstm</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="c1"># to_load = os.path.join('model', 'transformer_val_best_model')
</span>    <span class="c1"># model.load_state_dict(torch.load(to_load)['model_state_dict'])
</span>    <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
    <span class="n">pred_result</span><span class="p">,</span> <span class="n">true_result</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(),</span> <span class="nf">deque</span><span class="p">()</span>

    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>    
            <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span>  <span class="o">=</span> <span class="n">x_cat_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">is_lstm</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
            <span class="n">pred_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="n">pred_result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_batch</span><span class="p">)</span>
            <span class="n">true_result</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_batch</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>

    <span class="n">true_result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">true_result</span><span class="p">)</span>
    <span class="n">pred_result</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">pred_result</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="nf">classification_report</span><span class="p">(</span><span class="n">true_result</span><span class="p">,</span> <span class="n">pred_result</span><span class="p">))</span> 
    <span class="n">score</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">true_result</span><span class="p">,</span> <span class="n">pred_result</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">score</span>
    

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">'</span><span class="s">__main__</span><span class="sh">'</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">44</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">Xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">genfromtxt</span><span class="p">(</span><span class="nc">GzipFile</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sh">'</span><span class="s">covtype.data.gz</span><span class="sh">'</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">-=</span> <span class="mi">1</span>

        <span class="n">x_numers</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span>

        <span class="n">x_cats</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">10</span><span class="p">:]</span>
        <span class="n">x_cat1</span> <span class="o">=</span> <span class="n">x_cats</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">x_cat2</span> <span class="o">=</span> <span class="n">x_cats</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">:].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">x_cat</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">x_cat1</span><span class="p">,</span> <span class="n">x_cat2</span><span class="p">]).</span><span class="n">T</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>
        <span class="n">test_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
        <span class="n">val_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
        <span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_test</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_c_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">x_numers</span><span class="p">,</span> <span class="n">x_cat</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_ratio</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">44</span><span class="p">)</span>
        <span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_c_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">val_ratio</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">44</span><span class="p">)</span>

        <span class="n">preprocess_n</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">)</span>
        <span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">x_n_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_val</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_test</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_n_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
        <span class="n">val_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_val</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
        <span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_test</span><span class="p">,</span> <span class="n">x_n_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
        <span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>
        <span class="n">val_loader</span><span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>

        <span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">192</span>
        <span class="n">cft</span> <span class="o">=</span> <span class="nc">CatFeatureTokenizer</span><span class="p">(</span><span class="n">max_cats</span><span class="o">=</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">40</span><span class="p">],</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">nft</span> <span class="o">=</span> <span class="nc">NumerFeatureTokenizer</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">model</span> <span class="o">=</span> <span class="nc">TransformerClassifier</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">cat_tokenizer</span><span class="o">=</span><span class="n">cft</span><span class="p">,</span> <span class="n">numer_tokenizer</span><span class="o">=</span><span class="n">nft</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.0001</span>
        <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
        <span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="nc">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span><span class="p">)</span>

        <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
        <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">FileNotFoundError</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">Data not found.</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div> <p>As for <code class="language-plaintext highlighter-rouge">optuna.ipynb</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">math</span>
<span class="kn">import</span> <span class="n">optuna</span>
<span class="kn">import</span> <span class="n">torch</span>  <span class="c1"># torch.__version__ &gt;= 1.9.0
</span><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> 
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">accuracy_score</span> 
<span class="kn">from</span> <span class="n">gzip</span> <span class="kn">import</span> <span class="n">GzipFile</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span> 
<span class="kn">import</span> <span class="n">os</span> 
<span class="kn">from</span> <span class="n">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span> 
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>

<span class="kn">from</span> <span class="n">ft_transformer</span> <span class="kn">import</span> <span class="n">NumerFeatureTokenizer</span><span class="p">,</span> <span class="n">CatFeatureTokenizer</span><span class="p">,</span> <span class="n">TransformerClassifier</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="sh">'</span><span class="s">model</span><span class="sh">'</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">44</span><span class="p">)</span>
</code></pre></div></div> <p>Read and pre-process the data</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Xy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">genfromtxt</span><span class="p">(</span><span class="nc">GzipFile</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="sh">'</span><span class="s">covtype.data.gz</span><span class="sh">'</span><span class="p">),</span> <span class="n">delimiter</span><span class="o">=</span><span class="sh">"</span><span class="s">,</span><span class="sh">"</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Xy</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">].</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">y</span> <span class="o">-=</span> <span class="mi">1</span>

<span class="n">x_numers</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span>
<span class="n">x_cats</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">10</span><span class="p">:]</span>
<span class="n">x_cat1</span> <span class="o">=</span> <span class="n">x_cats</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">4</span><span class="p">].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">x_cat2</span> <span class="o">=</span> <span class="n">x_cats</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">:].</span><span class="nf">argmax</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">copy</span><span class="p">()</span>
<span class="n">x_cats</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">x_cat1</span><span class="p">,</span> <span class="n">x_cat2</span><span class="p">]).</span><span class="n">T</span><span class="p">.</span><span class="nf">copy</span><span class="p">()</span>

<span class="n">test_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">val_ratio</span> <span class="o">=</span> <span class="mf">0.2</span>
<span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_test</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_c_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">x_numers</span><span class="p">,</span> <span class="n">x_cats</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_ratio</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">44</span><span class="p">)</span>
<span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_c_val</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_val</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_c_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">val_ratio</span><span class="p">,</span> <span class="n">stratify</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">44</span><span class="p">)</span>

<span class="n">preprocess_n</span> <span class="o">=</span> <span class="nc">StandardScaler</span><span class="p">().</span><span class="nf">fit</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">)</span>
<span class="n">x_n_train</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">x_n_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_train</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_val</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">preprocess_n</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">x_n_test</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

</code></pre></div></div> <p>Dataloader:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">cats_data</span><span class="p">,</span> <span class="n">numers_data</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cats_data</span> <span class="o">=</span> <span class="n">cats_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">numers_data</span> <span class="o">=</span> <span class="n">numers_data</span>
        <span class="n">self</span><span class="p">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span> 
        <span class="n">self</span><span class="p">.</span><span class="n">length</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">cats_data</span><span class="p">)</span>
    

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">cats_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">numers_data</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">labels</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>


    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">length</span>   

<span class="n">train_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_train</span><span class="p">,</span> <span class="n">x_n_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_val</span><span class="p">,</span> <span class="n">x_n_val</span><span class="p">,</span> <span class="n">y_val</span><span class="p">)</span>
<span class="n">test_dataset</span> <span class="o">=</span> <span class="nc">CatNumerDataset</span><span class="p">(</span><span class="n">x_c_test</span><span class="p">,</span> <span class="n">x_n_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">val_loader</span><span class="o">=</span>  <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="c1">#  subset of dataset for tutorial
</span><span class="n">subset_train_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randperm</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">))[:</span><span class="mi">3000</span><span class="p">]</span>
<span class="n">subset_val_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randperm</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">))[:</span><span class="mi">3000</span><span class="p">]</span>
<span class="n">train_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">SubsetRandomSampler</span><span class="p">(</span><span class="n">subset_train_indices</span><span class="p">)</span>
<span class="n">val_sampler</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">SubsetRandomSampler</span><span class="p">(</span><span class="n">subset_val_indices</span><span class="p">)</span>
<span class="n">subset_train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">train_sampler</span><span class="p">)</span>
<span class="n">subset_val_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">sampler</span><span class="o">=</span><span class="n">val_sampler</span><span class="p">)</span>
</code></pre></div></div> <p>Supportive functions for <code class="language-plaintext highlighter-rouge">optuna</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">classes</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">in_features</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">max_cats</span> <span class="o">=</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">40</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">build_fttransformer</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">max_cats</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">192</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">embedding_dim</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">'</span><span class="s">embedding_dim</span><span class="sh">'</span><span class="p">,</span> <span class="mi">160</span><span class="p">,</span> <span class="mi">512</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_float</span><span class="p">(</span><span class="sh">'</span><span class="s">dropout</span><span class="sh">'</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
    
    <span class="n">cft</span> <span class="o">=</span> <span class="nc">CatFeatureTokenizer</span><span class="p">(</span><span class="n">max_cats</span><span class="o">=</span><span class="n">max_cats</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">nft</span> <span class="o">=</span> <span class="nc">NumerFeatureTokenizer</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">TransformerClassifier</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">cat_tokenizer</span><span class="o">=</span><span class="n">cft</span><span class="p">,</span> <span class="n">numer_tokenizer</span><span class="o">=</span><span class="n">nft</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="c1">#  MLP model
</span><span class="k">class</span> <span class="nc">MLPClassifier</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
                 <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
                 <span class="n">cat_tokenizer</span><span class="p">:</span> <span class="n">CatFeatureTokenizer</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">numer_tokenizer</span><span class="p">:</span> <span class="n">NumerFeatureTokenizer</span> <span class="o">=</span> <span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">MLPClassifier</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>

        <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span> <span class="o">=</span> <span class="n">cat_tokenizer</span>
        <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span> <span class="o">=</span> <span class="n">numer_tokenizer</span>
        
        <span class="k">if</span> <span class="n">cat_tokenizer</span> <span class="ow">and</span> <span class="n">numer_tokenizer</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span> <span class="o">==</span> <span class="n">numer_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="sh">'</span><span class="s">Inconsistent tokenizer dimensions</span><span class="sh">'</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">elif</span> <span class="n">cat_tokenizer</span><span class="p">:</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">cat_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">elif</span> <span class="n">numer_tokenizer</span><span class="p">:</span>
            <span class="n">d_model</span> <span class="o">=</span> <span class="n">numer_tokenizer</span><span class="p">.</span><span class="n">embedding_dim</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="nc">ValueError</span><span class="p">(</span><span class="sh">'</span><span class="s">At least one tokenizer is required</span><span class="sh">'</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">d_model</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_classes</span><span class="p">)</span>
        
        <span class="n">self</span><span class="p">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">))</span>
            <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">activation</span><span class="p">)</span>
            
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        x: (x_cats, x_numer) tuple; or x_cats/ x_numer tensor
        shape: [batch_size, feature_dim, embedding_dim]
        output: [batch_size, num_classes]
        </span><span class="sh">'''</span>
        <span class="k">if</span> <span class="nf">isinstance</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span> <span class="ow">and</span> <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span><span class="p">:</span>
                <span class="n">x_cats</span><span class="p">,</span> <span class="n">x_numer</span> <span class="o">=</span> <span class="n">x</span> 
                <span class="n">x_cats</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cat_tokenizer</span><span class="p">(</span><span class="n">x_cats</span><span class="p">)</span>
                <span class="n">x_numer</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">numer_tokenizer</span><span class="p">(</span><span class="n">x_numer</span><span class="p">)</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">x_cats</span><span class="p">,</span> <span class="n">x_numer</span><span class="p">])</span>  <span class="c1"># hstack available in version 1.8.0
#                 x = torch.cat([x_cats, x_numer], dim=1)
</span>            <span class="k">else</span><span class="p">:</span>
                <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Tokenizer not found</span><span class="sh">'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">cat_tokenizer</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cat_tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">numer_tokenizer</span><span class="p">:</span>
                <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">numer_tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  
        <span class="n">device</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">device</span>      
        
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">classifier</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span> 

<span class="k">def</span> <span class="nf">build_mlp</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">feature_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">192</span><span class="p">,</span> <span class="n">device</span> <span class="o">=</span> <span class="n">device</span><span class="p">):</span>
    
    <span class="n">cft</span> <span class="o">=</span> <span class="nc">CatFeatureTokenizer</span><span class="p">(</span><span class="n">max_cats</span><span class="o">=</span><span class="n">max_cats</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="n">nft</span> <span class="o">=</span> <span class="nc">NumerFeatureTokenizer</span><span class="p">(</span><span class="n">feature_dim</span><span class="o">=</span><span class="n">feature_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
    <span class="c1"># We optimize the number of layers.
</span>    <span class="n">n_layers</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_int</span><span class="p">(</span><span class="sh">"</span><span class="s">n_layers</span><span class="sh">"</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nc">MLPClassifier</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="n">n_layers</span><span class="p">,</span> <span class="n">cat_tokenizer</span><span class="o">=</span><span class="n">cft</span><span class="p">,</span> <span class="n">numer_tokenizer</span><span class="o">=</span><span class="n">nft</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">model</span>

<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    <span class="n">model_name</span> <span class="o">=</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_categorical</span><span class="p">(</span><span class="sh">'</span><span class="s">model_name</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">mlp</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">fttransformer</span><span class="sh">'</span><span class="p">])</span>
    <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">mlp</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">build_mlp</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">in_features</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">model_name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">fttransformer</span><span class="sh">'</span><span class="p">:</span>
        <span class="k">return</span> <span class="nf">build_fttransformer</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">classes</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">max_cats</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train_evaluate</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)):</span>
        <span class="n">model</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span>
        <span class="n">current_loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
            <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span>  <span class="o">=</span> <span class="n">x_cat_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">))</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>
            <span class="n">current_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">()</span>

        <span class="n">model</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span>
        <span class="n">val_score</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">()</span>
        <span class="n">pred_val</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">()</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span>
            <span class="k">for</span> <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span> <span class="ow">in</span> <span class="n">val_loader</span><span class="p">:</span>
            
                <span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">,</span> <span class="n">y_batch</span>  <span class="o">=</span> <span class="n">x_cat_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">pred</span> <span class="o">=</span> <span class="nf">model</span><span class="p">((</span><span class="n">x_cat_batch</span><span class="p">,</span> <span class="n">x_numer_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)))</span>
                <span class="n">pred_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
                <span class="n">pred_val</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">pred_batch</span><span class="p">)</span>
                <span class="n">val_score</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">y_batch</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">())</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">y_batch</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">int64</span><span class="p">))</span>
        
        <span class="n">val_score</span> <span class="o">=</span> <span class="nf">accuracy_score</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">val_score</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">(</span><span class="n">pred_val</span><span class="p">))</span>

        <span class="n">trial</span><span class="p">.</span><span class="nf">report</span><span class="p">(</span><span class="n">val_score</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trial</span><span class="p">.</span><span class="nf">should_prune</span><span class="p">():</span>
            <span class="k">raise</span> <span class="n">optuna</span><span class="p">.</span><span class="n">exceptions</span><span class="p">.</span><span class="nc">TrialPruned</span><span class="p">()</span>
            
    <span class="k">return</span> <span class="n">val_score</span> 

<span class="k">def</span> <span class="nf">objective</span><span class="p">(</span><span class="n">trial</span><span class="p">):</span>
    
    <span class="n">params</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">:</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_loguniform</span><span class="p">(</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">,</span> <span class="mf">1e-5</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">),</span>
        <span class="sh">'</span><span class="s">optimizer_name</span><span class="sh">'</span><span class="p">:</span> <span class="n">trial</span><span class="p">.</span><span class="nf">suggest_categorical</span><span class="p">(</span><span class="sh">'</span><span class="s">optimizer</span><span class="sh">'</span><span class="p">,</span> <span class="p">[</span><span class="sh">'</span><span class="s">AdamW</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">Adam</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">SGD</span><span class="sh">'</span><span class="p">]),</span>
    <span class="p">}</span>
    <span class="n">model</span> <span class="o">=</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">trial</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    
    <span class="n">optimizer</span> <span class="o">=</span> <span class="nf">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">optimizer_name</span><span class="sh">'</span><span class="p">])(</span><span class="n">model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">lr</span><span class="sh">'</span><span class="p">])</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">score</span> <span class="o">=</span> <span class="nf">train_evaluate</span><span class="p">(</span><span class="n">trial</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">subset_train_loader</span><span class="p">,</span> <span class="n">subset_val_loader</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">score</span> 

</code></pre></div></div> <p>Run experiments:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">study</span> <span class="o">=</span> <span class="n">optuna</span><span class="p">.</span><span class="nf">create_study</span><span class="p">(</span><span class="n">direction</span><span class="o">=</span><span class="sh">'</span><span class="s">maximize</span><span class="sh">'</span><span class="p">)</span>
<span class="n">study</span><span class="p">.</span><span class="nf">optimize</span><span class="p">(</span><span class="n">objective</span><span class="p">,</span> <span class="n">n_trials</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">timeout</span><span class="o">=</span><span class="mi">600</span><span class="p">)</span>

<span class="n">pruned_trials</span> <span class="o">=</span> <span class="n">study</span><span class="p">.</span><span class="nf">get_trials</span><span class="p">(</span><span class="n">deepcopy</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="p">[</span><span class="n">optuna</span><span class="p">.</span><span class="n">trial</span><span class="p">.</span><span class="n">TrialState</span><span class="p">.</span><span class="n">PRUNED</span><span class="p">])</span>
<span class="n">complete_trials</span> <span class="o">=</span> <span class="n">study</span><span class="p">.</span><span class="nf">get_trials</span><span class="p">(</span><span class="n">deepcopy</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">states</span><span class="o">=</span><span class="p">[</span><span class="n">optuna</span><span class="p">.</span><span class="n">trial</span><span class="p">.</span><span class="n">TrialState</span><span class="p">.</span><span class="n">COMPLETE</span><span class="p">])</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Study statistics: </span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  Number of finished trials: </span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">study</span><span class="p">.</span><span class="n">trials</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  Number of pruned trials: </span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">pruned_trials</span><span class="p">))</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  Number of complete trials: </span><span class="sh">"</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">complete_trials</span><span class="p">))</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Best trial:</span><span class="sh">"</span><span class="p">)</span>
<span class="n">trial</span> <span class="o">=</span> <span class="n">study</span><span class="p">.</span><span class="n">best_trial</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  Value: </span><span class="sh">"</span><span class="p">,</span> <span class="n">trial</span><span class="p">.</span><span class="n">value</span><span class="p">)</span>

<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">  Params: </span><span class="sh">"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">trial</span><span class="p">.</span><span class="n">params</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
    <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">    {}: {}</span><span class="sh">"</span><span class="p">.</span><span class="nf">format</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>
</code></pre></div></div> <p>Check my post on Discovery Lab: <a href="https://mp.weixin.qq.com/s/iFX9CiEFmIn9hARKIwGtUQ" rel="external nofollow noopener" target="_blank">【每周一读】重提表格数据上的深度学习</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-more-mcts/">Some Recent Advancement Around MuZero</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-mpc/">MPC with a Differentiable Forward Model: An Implementation with Jax</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-muax/">Adding MuZero into RL Toolkits at Ease</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/blog-hindsight/">“Hindsight” – An easy yet effective RL Technique HER with Pytorch implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/blog-drlhft/">Will DRL Make Profit in High-Frequency Trading?</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bowen Fang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>