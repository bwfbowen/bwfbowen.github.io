<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> “Hindsight” – An easy yet effective RL Technique HER with Pytorch implementation | Bowen Fang </title> <meta name="author" content="Bowen Fang"> <meta name="description" content="The personal academic website of Bowen Fang. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%95&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://bwfbowen.github.io/blog/2022/blog-hindsight/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Bowen Fang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">More </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/blog/">blog</a> <a class="dropdown-item " href="/publications/">publications</a> <a class="dropdown-item " href="/cv/">cv</a> <a class="dropdown-item " href="/projects/">projects</a> <a class="dropdown-item " href="/repositories/">repositories</a> <a class="dropdown-item " href="/teaching/">teaching</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">“Hindsight” – An easy yet effective RL Technique HER with Pytorch implementation</h1> <p class="post-meta"> Created on May 18, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/her"> <i class="fa-solid fa-hashtag fa-sm"></i> HER</a>   <a href="/blog/tag/pytorch"> <i class="fa-solid fa-hashtag fa-sm"></i> Pytorch</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>This week, I will share a paper published by OpenAI at NeurIPS 2017. The ideas presented in this paper are quite insightful, and it tackles a complex problem using only simple algorithmic improvements. I gained significant inspiration from this paper. At the end, I will also provide a brief implementation of HER (Hindsight Experience Replay).</p> <blockquote> <p>Original Paper Information <br> <strong>Title</strong>: Hindsight experience replay <br> <strong>Author</strong>: Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, Wojciech Zaremba <br> <strong>Code</strong>: <a href="https://github.com/openai/baselines/blob/master/baselines/her/README.md" rel="external nofollow noopener" target="_blank">https://github.com/openai/baselines/blob/master/baselines/her/README.md</a></p> </blockquote> <h1 id="background">Background</h1> <p>The combination of reinforcement learning and neural networks has achieved success in various domains involving sequential decision-making, such as Atari games, Go, and robot control tasks.</p> <p>Typically, in reinforcement learning tasks, a crucial aspect is designing a reward function that reflects the task itself and guides the optimization of the policy. Designing a reward function can be highly complex, which limits the application of reinforcement learning in the real world. It requires not only understanding of the algorithm itself but also substantial domain-specific knowledge. Moreover, in scenarios where it is difficult for us to determine what actions are appropriate, it is challenging to design an appropriate reward function. Therefore, algorithms that can learn policies from rewards that do not require explicit design, such as binary variables indicating task completion, are important for applications.</p> <p>One capability that humans possess but most model-free reinforcement learning algorithms lack is the ability to learn from “hindsight”. For example, if a basketball shot misses to the right, a reinforcement learning algorithm would conclude that the sequence of actions associated with the shot is unlikely to lead to success. However, an alternative “hindsight” conclusion can be drawn, namely that if the basket were slightly to the right (to the location where the ball landed), the same sequence of actions would result in success.</p> <p>The paper introduces a technique called Hindsight Experience Replay (HER), which can be combined with any off-policy RL algorithm. HER not only improves sampling efficiency but, more importantly, enables the algorithm to learn policies from binary and sparse rewards. HER incorporates the current state and a target state as inputs during replay, where the core idea is to replay an episode (a complete trajectory) using a different goal than the one the agent was originally trying to achieve.</p> <h1 id="her">HER</h1> <h2 id="1-an-introducing-example">1. An introducing example</h2> <p>Consider a coin flipping problem with a total of $n$ coins. The configuration of heads or tails for these $n$ coins represents a state, and the state space is denoted as $S={0,1}^n$. The action space is $A={0,1,…,n-1}$, where $i$ denotes flipping the i-th coin. In each episode, an initial state and a target state are uniformly and randomly selected. The RL policy flips these $n$ coins, and if the resulting state differs from the target state, a reward of $-1$ is obtained, i.e., $r_g(s,a)=-[s\neq g]$. Here, $g$ represents the target state.</p> <p>When $n&gt;40$, reinforcement learning strategies almost always fail because the policy rarely encounters rewards other than $-1$. Relying on random action exploration in such a large state space is impractical. A common approach in reinforcement learning is to design a reward function that guides the policy towards the goal. In this example, $r_g(s,a)=|s-g|^2$ can solve the problem. However, designing a reward function can be challenging, especially when facing more complex problems.</p> <p>The solution proposed in this paper does not require any domain-specific knowledge. Consider an episode that goes through a sequence of states $s_1, s_2, …, s_T$, and a target $g\neq s_1, s_2, …, s_T$ that has not been achieved. In each step of this episode, a reward of $-1$ is obtained. The method described in the paper involves replacing $g$ with $s_T$ and adding the modified episode to the replay buffer. This approach introduces paths with rewards different from $-1$, making the learning process simpler.</p> <p>Figure 1 compares the performance of plain DQN and DQN+HER in this environment. DQN without HER can solve problems with at most 13 coins, while DQN+HER can easily handle environments with 50 coins.</p> <p><img src="/assets/img/bit-flipping.png" alt="bit-flipping" width="500" height="300" style="margin-left: auto; margin-right: auto; display: block;"></p> <figure> <figcaption style="text-align: center">Figure 1. Bit-flipping experiment w/o HER </figcaption> </figure> <h2 id="2-multi-goals-scenario">2. Multi-goals scenario</h2> <p>The problem scenario of interest involves an agent capable of achieving multiple distinct goals. Let’s assume there exists a set of goals $G$, where each goal $g\in G$ has a corresponding mapping $f_g:S\rightarrow {0,1}$, and the agent’s objective is to reach any state $s$ for which $f_g(s)=1$. For example, a goal could be a specific state itself: $f_g(s)=[s=g]$, or a goal related to a certain property of the state, such as reaching a given x-coordinate in a two-dimensional coordinate system: $f_g((x,y))=[x=g]$.</p> <p>Furthermore, the paper assumes that given a state $s$, it is straightforward to find a corresponding goal $g$. In other words, there exists a mapping $m:S\rightarrow G$ such that for every $s\in S$, $f_{m(s)}(s)=1$. For instance, in the case of a two-dimensional coordinate system, $m((x, y))=x$.</p> <p>Using a binary sparse reward function that assigns $-1$ if the goal is not achieved at each time step and 0 if it is achieved does not yield good results during actual training. This is because the reward is too sparse and lacks sufficient information. To address this problem, the original paper proposes the HER algorithm.</p> <h2 id="3-her-algorithm">3. HER algorithm</h2> <p>The idea behind HER is quite simple: after experiencing a sequence of states $s_1, s_2, …, s_T$, each transition $s_t\rightarrow s_{t+1}$ is saved. These transitions are not only associated with the original goal that generated the sequence but also grouped with a subset of other goals. Different goals only affect the agent’s actions and not the transition probabilities of the entire environment, allowing off-policy algorithms to use different goals during training.</p> <p>With HER, one task is to define the “subset of other goals.” The simplest approach, $m(s_T)$, involves using only the final state of a single trajectory as a goal.</p> <figure> <img src="/assets/img/her_algo.png" alt="HER algorithm"> <figcaption style="text-align: center">Figure 2. HER algorithm </figcaption> </figure> <p>It’s worth noting that the algorithm of HER differs from a standard off-policy algorithm in a couple of ways. Firstly, the policy’s input is a concatenation of the state vector $s$ and the goal vector $g$. Secondly, the Replay Buffer stores not only the transition information generated through interactions with the environment but also the transition information after replacing the goal with a new goal $g’$. These pieces of information are collectively used for subsequent training.</p> <h1 id="experiment">Experiment</h1> <p>The paper’s experiments can be referenced in the following <a href="https://sites.google.com/site/hindsightexperiencereplay/" rel="external nofollow noopener" target="_blank">video</a>:</p> <p>The organization of the experimental section in the paper is as follows: The first part introduces the reinforcement learning environments used. Subsequently, each section explores the performance differences between DDPG with and without HER, the performance in single-goal scenarios, the impact of designing additional reward functions on performance, the influence of different sampling methods of goals on policy performance, and finally, the deployment of the algorithm on real robots.</p> <h2 id="1-environment">1. Environment</h2> <p>The paper used <code class="language-plaintext highlighter-rouge">Mujoco</code> and introduced its own environment, which has been made publicly available in the <a href="https://gym.openai.com/envs/#robotics" rel="external nofollow noopener" target="_blank">OpenAI Gym</a></p> <figure> <img src="/assets/img/mujoco_her.gif" alt="environments used"> <figcaption style="text-align: center">Figure 3. Experiment environments </figcaption> </figure> <p>The policy is represented using a MLP with ReLU activation. The training utilizes the DDPG algorithm with the Adam optimizer.</p> <p>There are three tasks:</p> <ul> <li> <p><strong>Pushing</strong>: Pushing a box to a specified position on a table without performing the “grasp” action.</p> </li> <li> <p><strong>Slide</strong>: Sliding a ball to a target position on a smooth table that is outside the arm’s range of motion.</p> </li> <li> <p><strong>Pick and Place</strong>: Picking up a box and placing it at a designated airborne position.</p> </li> </ul> <p><strong>State</strong>:</p> <p>The state is represented by the Mujoco physics engine, including the angles and velocities of each robotic joint, as well as the positions, rotations, angular momentum, and linear momentum of all objects.</p> <p><strong>Goal</strong>:</p> <p>The goal is the desired position of the object, with a fixed tolerance $\epsilon$. For example, $G=R^3$, where $f_g(s)=[|g-s_{object}|\leq\epsilon]$, and $s_{object}$ represents the position of the object in state $s$. The mapping function is defined as $m(s)=s_{object}$.</p> <p><strong>Reward</strong>:</p> <p>Except for the section on <em>Reward Design Comparison</em>, the reward is defined as $r(s,a,g)=-[f_g(s’)=0]$, where $s’$ is the resulting state after performing action $a$ in state $s$.</p> <p><strong>Action</strong>:</p> <p>The action is four-dimensional, with the first three dimensions representing the desired relative position of the robotic gripper in the next time step, and the last dimension representing the desired distance between the two robotic fingers.</p> <h2 id="2-can-her-improve-performance">2. Can HER improve performance?</h2> <p>As shown in Figure 4, the performance of DDPG, DDPG with count-based exploration (Strehl and Littman, 2005; Kolter and Ng, 2009; Tang et al., 2016; Bellemare et al., 2016; Ostrovski et al., 2017), and DDPG+HER is compared. The blue line represents HER, which stores the transitions twice - once with the goal that generates interaction data with the environment and once with the goal changed to the state reached at the end of the episode. The red line is the best-performing HER strategy among the different goal selection strategies discussed in Section 5.</p> <figure> <img src="/assets/img/her_ddpg_multi_cmp.png" alt="HER performance comparison"> <figcaption style="text-align: center">Figure 4. HER performance comparison, where the average results are taken from 5 random seeds, and the shadow represents one standard deviation. The red line represents the strategy using future sampling with k=4. </figcaption> </figure> <p>From Figure 4, we can observe that vanilla DDPG fails to learn the policies for all three tasks. DDPG with count-based exploration shows some progress only in the sliding task. However, DDPG+HER excels in solving all three tasks. It can be concluded that HER is crucial for learning policies from sparse and binary rewards.</p> <h2 id="3-can-her-improve-performance-when-there-is-only-one-goal">3. Can HER improve performance when there is only one goal?</h2> <p>In this section’s experiment, the goal for each episode is fixed to be the same (the same target position), repeating the experiment from Section 2. From Figure 5, it can be observed that DDPG+HER outperforms vanilla DDPG. Comparing Figure 4 and Figure 5, it is evident that HER learns faster when there are multiple goals. Therefore, the authors suggest that even if there is only one specific goal of interest, training can still be conducted using a multi-goal approach.</p> <figure> <img src="/assets/img/ddpg_her_single_cmp.png" alt="HER performance comparison"> <figcaption style="text-align: center">Figure 5. HER single goal performance comparison </figcaption> </figure> <h2 id="4-how-does-her-interact-with-the-design-of-reward-functions">4. How does HER interact with the design of reward functions?</h2> <p>In this part, an attempt is made to design a reward function instead of using the binary sparse reward employed in the previous experiments. A shaped reward is considered: $r(s,a,g)=\lambda |g-s_{object} |^p-|g-s_{object}’ |^p$, where $s’$ represents the state reached after performing action $a$ in state $s$; $\lambda\in{0,1}$ and $p\in{1, 2}$ are hyperparameters.</p> <figure> <img src="/assets/img/her_reng.png" alt="HER performance comparison"> <figcaption style="text-align: center">Figure 6. Performance comparison between HER and HER with designed reward functions. </figcaption> </figure> <p>From Figure 6, it can be observed that both vanilla DDPG and DDPG+HER did not learn effective policies. (This is possibly resulted from the fact that when applying reinforcement learning to complex manipulation tasks, it often requires designing reward functions that are much more complex than the ones attempted by the authors.)</p> <p>The authors believe that the failures stem from two reasons: 1. The optimization objective (shaped reward) differs significantly from the condition for success (whether the final position is within a certain radius of the target). 2. The shaped reward penalizes inappropriate actions, hindering exploration.</p> <p>While more complex reward designs may potentially address the issue, they require a substantial amount of domain knowledge. This fact reinforces the importance highlighted in the paper of effective learning from binary sparse rewards.</p> <h2 id="5-how-many-goals-should-be-chosen-and-how-should-they-be-selected">5. How many goals should be chosen and how should they be selected?</h2> <p>In previous experiments, only the final state of an episode was utilized. In this section, additional goal sampling methods are considered. The following methods are taken into account:</p> <ol> <li> <strong>Final</strong>: The final state of an episode.</li> <li> <strong>Random</strong>: Randomly select $k$ states encountered throughout the training process.</li> <li> <strong>Episode</strong>: Randomly select $k$ states encountered within a single episode.</li> <li> <strong>Future</strong>: Randomly select $k$ states that appear after the current state within the same episode.</li> </ol> <figure> <img src="/assets/img/goal_strats.png" alt="HER performance comparison"> <figcaption style="text-align: center">Figure 7. Performance comparison of different goal selection methods. </figcaption> </figure> <p>As shown in Figure 7, the top row represents the highest success rate, while the bottom row represents the average success rate. The parameter $k$ controls the ratio between HER data in the Replay Buffer and the data generated by the agent’s interaction with the environment.</p> <p>It can be observed that for the third task, all goal selection methods except for Random perform remarkably well. Future combined with $k=4$ or $8$ shows the best performance across all tasks and is the only method that performs well in the second task. However, when $k$ exceeds $8$, there is a decline in performance, as the proportion of data generated by the agent’s interaction with the environment becomes relatively low.</p> <h2 id="6-deploy-on-a-real-robot">6. Deploy on a real robot</h2> <p>Lastly, the authors conducted an experiment where the algorithm (Future, $k=4$) was deployed on a real robot. They added a separately trained CNN layer to recognize camera images. In the initial five trials, the algorithm succeeded twice. However, after introducing noise to the observed data, all five trials were successful.</p> <h1 id="conclusion">Conclusion</h1> <p>The contributions of this paper is that:</p> <p>This paper proposes an innovative method that enables reinforcement learning algorithms to learn complex task policies from simple binary sparse rewards. This approach can be combined with any off-policy algorithm.</p> <h1 id="review-comments">Review comments</h1> <p>From <a href="https://proceedings.neurips.cc/paper/2017/file/453fadbd8a1a3af50a9df4df899537b5-Reviews.html" rel="external nofollow noopener" target="_blank">NeurIPS review</a>, one can see the questions raised by the reviewers regarding the original paper before its acceptance. Among them, one reasonable suggestion is to explore the combination of HER with a proven effective reward function for evaluating the performance of the combination of HER with reward engineering.</p> <h1 id="pytorch-implementation">Pytorch implementation</h1> <p>The article is very clear, with interesting ideas and a simple implementation. I personally really like this paper.</p> <p>Moreover, I attempted to implement HER myself, applying it to the <code class="language-plaintext highlighter-rouge">MountainCar-v0</code> environment in the Gym framework. In this environment, there is a single goal, and a reward of $-1$ is received at each time step until the goal is achieved. Without using HER, the typical approach is to rely on a large number of random actions to discover successful paths or to redesign the reward function. However, with DQN+HER, the policy can be learned relatively quickly, while vanilla DQN requires more time steps and more random exploration.</p> <figure> <img src="/assets/img/mountaincar-her.gif" alt="mountain car HER"> <figcaption style="text-align: center">Mountain Car HER </figcaption> </figure> <p>So first import the necessary libraries:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">os</span> 
<span class="kn">import</span> <span class="n">gym</span> 
<span class="kn">from</span> <span class="n">typing</span> <span class="kn">import</span> <span class="n">Tuple</span>
<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span> 
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="n">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">deque</span>
<span class="kn">from</span> <span class="n">itertools</span> <span class="kn">import</span> <span class="n">chain</span> 
<span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">copy</span> <span class="kn">import</span> <span class="n">deepcopy</span>

<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">'</span><span class="s">cpu</span><span class="sh">'</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">'</span><span class="s">cuda:0</span><span class="sh">'</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
<span class="n">os</span><span class="p">.</span><span class="nf">makedirs</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">drl</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">DQN</span><span class="sh">'</span><span class="p">),</span> <span class="n">exist_ok</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <p>We define an MLP policy:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">MLPNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="k">if</span> <span class="n">hidden_size</span> <span class="k">else</span> <span class="n">input_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">output_size</span> <span class="o">=</span> <span class="n">output_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ModuleList</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">ReLU</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_layers</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">first</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">first</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">activation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">hidden</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">hidden</span><span class="p">)</span>
                <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">activation</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">classifier</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">classifier</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">mlp</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="nf">module</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span> 


<span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">is_discrete</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span> <span class="k">if</span> <span class="n">is_discrete</span> <span class="k">else</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="nc">MLPNet</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">mlp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</code></pre></div></div> <p>And Replay Buffer:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">replay_buffer_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">type</span> <span class="o">=</span> <span class="sh">'</span><span class="s">RB</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
        <span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span> <span class="o">=</span> <span class="n">replay_buffer_capacity</span>
        <span class="n">self</span><span class="p">.</span><span class="n">states</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">next_states</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dones</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer_capacity</span><span class="p">)</span>
        

    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">states</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__getitem__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="nf">return </span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">states</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">next_states</span><span class="p">[</span><span class="n">index</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">dones</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
            <span class="p">)</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trans</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        trans: (state, action, reward, next_state, done)
        follows OpenAI gym
        </span><span class="sh">'''</span>
        <span class="n">self</span><span class="p">.</span><span class="n">states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">actions</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">rewards</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">next_states</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dones</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
</code></pre></div></div> <p>And HER:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HER</span><span class="p">(</span><span class="n">ReplayBuffer</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">reward_func</span><span class="p">,</span> <span class="n">m_func</span><span class="p">,</span> <span class="n">sample_strategy</span> <span class="o">=</span> <span class="sh">'</span><span class="s">final</span><span class="sh">'</span><span class="p">,</span> <span class="n">k_goals</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">replay_buffer_capacity</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">(</span><span class="n">replay_buffer_capacity</span><span class="o">=</span><span class="n">replay_buffer_capacity</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">reward_func</span> <span class="o">=</span> <span class="n">reward_func</span>
        <span class="n">self</span><span class="p">.</span><span class="n">m_func</span> <span class="o">=</span> <span class="n">m_func</span>
        <span class="k">if</span> <span class="n">sample_strategy</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="sh">'</span><span class="s">final</span><span class="sh">'</span><span class="p">,</span> <span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="p">]:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="n">sample_strategy</span><span class="si">}</span><span class="s"> is not in [</span><span class="sh">'</span><span class="s">final</span><span class="sh">'</span><span class="s">, </span><span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="s">]</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">sample_strategy</span> <span class="o">=</span> <span class="n">sample_strategy</span>
        <span class="n">self</span><span class="p">.</span><span class="nb">type</span> <span class="o">=</span> <span class="sh">'</span><span class="s">HER</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">k_goals</span> <span class="o">=</span> <span class="n">k_goals</span>

    <span class="k">def</span> <span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">trans</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">goal</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        goal is a vector
        </span><span class="sh">'''</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">trans</span>
        <span class="n">goal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">goal</span><span class="p">)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_goal_size</span><span class="p">(</span><span class="n">goal</span><span class="p">)</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">s</span><span class="p">,</span> <span class="n">goal</span><span class="p">])</span>
        <span class="n">r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward_func</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="o">-</span><span class="n">size</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="n">size</span><span class="p">:])</span>
        <span class="n">ns</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">ns</span><span class="p">,</span> <span class="n">goal</span><span class="p">])</span>
        <span class="c1"># print(f'append:{(s, a, r, ns, d)}')
</span>        <span class="nf">super</span><span class="p">().</span><span class="nf">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
    
    <span class="k">def</span> <span class="nf">hindsight</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ep_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        At the end of episode, append hindsight
        </span><span class="sh">'''</span>
        <span class="c1"># print(f'ep_len: {ep_len}')
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">sample_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">final</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">g</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">m_func</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">R_</span><span class="p">,</span> <span class="n">tail</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k_goals</span><span class="p">):</span>
                <span class="n">R_tmp</span><span class="p">,</span> <span class="n">tail</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">hindsight_append</span><span class="p">(</span><span class="n">ep_len</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">tail</span><span class="p">)</span>
                <span class="n">R_</span> <span class="o">+=</span> <span class="n">R_tmp</span>
            <span class="n">R_</span> <span class="o">/=</span> <span class="n">self</span><span class="p">.</span><span class="n">k_goals</span>
            
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">sample_strategy</span> <span class="o">==</span> <span class="sh">'</span><span class="s">random</span><span class="sh">'</span><span class="p">:</span>
            <span class="n">R_</span><span class="p">,</span> <span class="n">tail</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">k_goals</span><span class="p">):</span>
                <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">self</span><span class="p">))</span>
                <span class="n">g</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">m_func</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
                <span class="n">R_</span><span class="p">,</span> <span class="n">tail</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">hindsight_append</span><span class="p">(</span><span class="n">ep_len</span><span class="p">,</span> <span class="n">g</span><span class="p">,</span> <span class="n">tail</span><span class="p">)</span>
            <span class="n">R_</span> <span class="o">/=</span> <span class="n">self</span><span class="p">.</span><span class="n">k_goals</span>
        <span class="k">return</span> <span class="n">R_</span>
    
    <span class="k">def</span> <span class="nf">hindsight_append</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">ep_len</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">,</span> <span class="n">tail</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
        <span class="n">size</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_goal_size</span><span class="p">(</span><span class="n">g</span><span class="p">)</span>
        <span class="n">R_</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ep_len</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="nf">deepcopy</span><span class="p">(</span><span class="n">self</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">tail</span><span class="p">)])</span>
            <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="n">size</span><span class="p">:]</span> <span class="o">=</span> <span class="n">g</span> 
            <span class="n">ns</span><span class="p">[</span><span class="o">-</span><span class="n">size</span><span class="p">:]</span> <span class="o">=</span> <span class="n">g</span>
            <span class="n">r</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">reward_func</span><span class="p">(</span><span class="n">s</span><span class="p">[:</span><span class="o">-</span><span class="n">size</span><span class="p">],</span> <span class="n">a</span><span class="p">,</span> <span class="n">s</span><span class="p">[</span><span class="o">-</span><span class="n">size</span><span class="p">:])</span>
            <span class="c1"># print(f'hindsight_append:{(s, a, r, ns, d)}')
</span>            <span class="nf">super</span><span class="p">().</span><span class="nf">append</span><span class="p">((</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
            <span class="n">tail</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">R_</span> <span class="o">+=</span> <span class="n">r</span> 
        
        <span class="k">return</span> <span class="n">R_</span><span class="p">,</span> <span class="n">tail</span>
    
    <span class="k">def</span> <span class="nf">_goal_size</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">g</span><span class="p">:</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">g</span><span class="p">.</span><span class="n">shape</span><span class="p">:</span>  <span class="c1"># np.array(1).shape -&gt; ()
</span>            <span class="n">size</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">size</span> <span class="o">=</span> <span class="n">g</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># np.array([1]).shape -&gt; (1,)
</span>        <span class="k">return</span> <span class="n">size</span>
</code></pre></div></div> <p>DQN:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BaseAgent</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">base</span><span class="sh">'</span>

    <span class="k">def</span> <span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">type</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">!=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="k">pass</span> 
    
<span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">BaseAgent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="n">goal_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">replay_buffer</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span>
                 <span class="n">gamma</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">epsilon</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.8</span><span class="p">,</span>
                 <span class="n">tau</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
                 <span class="n">cost</span><span class="p">:</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">modules</span><span class="p">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="sh">'</span><span class="s">mean</span><span class="sh">'</span><span class="p">),</span>
                 <span class="n">lr</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">,</span>
                 <span class="n">gradient_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
                 <span class="o">**</span><span class="n">kwargs</span>
                 <span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">DQN</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state_size</span> <span class="o">=</span> <span class="n">state_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_size</span> <span class="o">=</span> <span class="n">action_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">replay_buffer</span> <span class="k">if</span> <span class="n">replay_buffer</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="n">replay_buffer_capacity</span><span class="o">=</span><span class="mi">20000</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">is_her</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">.</span><span class="nb">type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">HER</span><span class="sh">'</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="k">assert</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">epsilon</span> <span class="o">&lt;</span> <span class="mi">1</span>
        <span class="n">self</span><span class="p">.</span><span class="n">tau</span> <span class="o">=</span> <span class="n">tau</span>
        <span class="n">self</span><span class="p">.</span><span class="n">cost</span> <span class="o">=</span> <span class="n">cost</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">main_network</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">is_discrete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">(</span><span class="n">state_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">is_discrete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">goal_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> 
            <span class="n">self</span><span class="p">.</span><span class="n">goal_size</span> <span class="o">=</span> <span class="n">goal_size</span>
            <span class="n">self</span><span class="p">.</span><span class="n">main_network</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">(</span><span class="n">state_size</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">is_discrete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">target_network</span> <span class="o">=</span> <span class="nc">Net</span><span class="p">(</span><span class="n">state_size</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">goal_size</span><span class="p">,</span> <span class="n">action_size</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="n">is_discrete</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">main_network</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">gradient_steps</span> <span class="o">=</span> <span class="n">gradient_steps</span>

    <span class="k">def</span> <span class="nf">store_transition</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="sh">'''</span><span class="s">
        s, a, r, ns, d, (g if her)
        </span><span class="sh">'''</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
            <span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">args</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
            <span class="n">trans_g</span> <span class="o">=</span> <span class="p">[</span><span class="o">*</span><span class="n">args</span><span class="p">]</span>
            <span class="n">trans</span><span class="p">,</span> <span class="n">g</span> <span class="o">=</span> <span class="n">trans_g</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">trans_g</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">trans</span><span class="p">,</span> <span class="n">g</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">self</span><span class="p">.</span><span class="n">epsilon</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">action_size</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="nf">main_network</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="nf">tolist</span><span class="p">()</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">goal</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span>
                    <span class="n">self</span><span class="p">.</span><span class="nf">main_network</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="p">]))).</span><span class="nf">tolist</span><span class="p">()</span>
                <span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">best_q_action</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">main_network</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">state</span><span class="p">)).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">goal</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">goal</span><span class="p">)</span>
            <span class="n">s_g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">hstack</span><span class="p">([</span><span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="p">])</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span>
                <span class="n">self</span><span class="p">.</span><span class="nf">main_network</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">s_g</span><span class="p">)).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">cpu</span><span class="p">().</span><span class="nf">numpy</span><span class="p">()</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">action</span>

    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">batch_size</span><span class="p">:</span>
            <span class="k">return</span> 
        <span class="n">loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nc">DataLoader</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="c1"># minibatch = random.sample(self.replay_buffer, batch_size)
</span>        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">gradient_steps</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">ns</span><span class="p">,</span> <span class="n">done</span> <span class="o">=</span> <span class="nf">next</span><span class="p">(</span><span class="nf">iter</span><span class="p">(</span><span class="n">loader</span><span class="p">))</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">ns</span> <span class="o">=</span> <span class="n">ns</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="c1"># print(done)
</span>            <span class="n">max_tensor</span><span class="p">,</span> <span class="n">max_idx</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">target_network</span><span class="p">(</span><span class="n">ns</span><span class="p">).</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">target_Q</span> <span class="o">=</span> <span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">done</span><span class="p">.</span><span class="nf">long</span><span class="p">())</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">max_tensor</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">Q_values</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">main_network</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="n">Q_targets</span> <span class="o">=</span> <span class="n">Q_values</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
            <span class="n">Q_targets</span><span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">Q_targets</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)),</span> <span class="n">a</span><span class="p">]</span> <span class="o">=</span> <span class="n">target_Q</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">cost</span><span class="p">(</span><span class="n">Q_values</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">Q_targets</span><span class="p">)</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span>
            <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>
            <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_target_network</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">main_network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

    <span class="k">def</span> <span class="nf">_preprocess_state</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="n">state</span><span class="p">):</span>
        <span class="k">if</span> <span class="nf">type</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">!=</span> <span class="n">torch</span><span class="p">.</span><span class="n">Tensor</span><span class="p">:</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span>
            <span class="p">{</span><span class="sh">'</span><span class="s">main_network_sd</span><span class="sh">'</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="n">main_network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">()},</span> <span class="n">path</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">main_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="n">path</span><span class="p">)[</span><span class="sh">'</span><span class="s">main_network_sd</span><span class="sh">'</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">target_network</span><span class="p">.</span><span class="nf">load_state_dict</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">main_network</span><span class="p">.</span><span class="nf">state_dict</span><span class="p">())</span>

</code></pre></div></div> <p>The training and evaluating functions:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">agent</span><span class="p">:</span> <span class="n">DQN</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span><span class="p">,</span> <span class="n">update_every</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">50</span><span class="p">,</span> <span class="n">require_render</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">is_her</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">require_epsilon_explore</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">save_path</span> <span class="o">=</span> <span class="n">save_path</span> <span class="k">if</span> <span class="n">save_path</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span> <span class="k">else</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">drl</span><span class="sh">'</span><span class="p">,</span> <span class="n">agent</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="n">time_step</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">cur_best</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="n">inf</span>
    <span class="n">Return_list</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">if</span> <span class="n">agent</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
        <span class="n">is_her</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">if</span> <span class="n">is_her</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">goal</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="sh">'</span><span class="s">goal</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">'</span><span class="s">No env goal has been provided.</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">agent</span><span class="p">.</span><span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">DQN</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">if</span> <span class="n">require_epsilon_explore</span><span class="p">:</span>
            <span class="n">epsilon_decay</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">-</span> <span class="mf">0.2</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">):</span>
            <span class="n">Return</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
            <span class="c1"># Cart-Pole
</span>            <span class="c1"># if np.mean(Return_list[-10:]) == 200:
</span>            <span class="c1">#     break
</span>            <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">):</span>
                <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">require_render</span><span class="p">:</span>
                    <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
                <span class="n">time_step</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="k">if</span> <span class="n">time_step</span> <span class="o">%</span> <span class="n">agent</span><span class="p">.</span><span class="n">tau</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">agent</span><span class="p">.</span><span class="nf">update_target_network</span><span class="p">()</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_her</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
                <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">is_her</span><span class="p">:</span>
                    <span class="n">agent</span><span class="p">.</span><span class="nf">store_transition</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">agent</span><span class="p">.</span><span class="nf">store_transition</span><span class="p">(</span><span class="n">state</span> <span class="p">,</span><span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
                <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
                <span class="n">Return</span> <span class="o">+=</span> <span class="n">reward</span>
                <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">is_her</span><span class="p">:</span>
                        <span class="n">Return_</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="n">replay_buffer</span><span class="p">.</span><span class="nf">hindsight</span><span class="p">(</span><span class="n">t</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
                        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Episode:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, Return_her:</span><span class="si">{</span><span class="n">Return_</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
                    <span class="n">Return_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">Return</span><span class="p">)</span>
                    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Episode:</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s">, Return:</span><span class="si">{</span><span class="n">Return</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">Return</span> <span class="o">&gt;=</span> <span class="n">cur_best</span><span class="p">:</span>
                        <span class="n">agent</span><span class="p">.</span><span class="nf">save</span><span class="p">(</span><span class="n">save_path</span><span class="p">)</span>
                        <span class="n">cur_best</span> <span class="o">=</span> <span class="n">Return</span>
                    <span class="k">break</span>
                <span class="k">if</span> <span class="n">steps</span> <span class="o">%</span> <span class="n">update_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">agent</span><span class="p">.</span><span class="nf">train</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">require_epsilon_explore</span><span class="p">:</span>
                <span class="n">agent</span><span class="p">.</span><span class="n">epsilon</span> <span class="o">-=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">num_episodes</span><span class="p">)</span> <span class="o">*</span> <span class="n">epsilon_decay</span> <span class="c1"># more exploitation
</span>        <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">Return_list</span>
    
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> <span class="n">require_render</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span>    
    <span class="n">Return</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">agent</span><span class="p">.</span><span class="n">is_her</span><span class="p">:</span>
        <span class="n">is_her</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">is_her</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="n">is_her</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">goal</span> <span class="o">=</span> <span class="n">kwargs</span><span class="p">[</span><span class="sh">'</span><span class="s">goal</span><span class="sh">'</span><span class="p">]</span>
        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nc">ValueError</span><span class="p">(</span><span class="sh">'</span><span class="s">No env goal has been provided.</span><span class="sh">'</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">agent</span><span class="p">.</span><span class="n">name</span> <span class="o">==</span> <span class="sh">'</span><span class="s">DQN</span><span class="sh">'</span><span class="p">:</span>
        <span class="c1"># best action
</span>        
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_timesteps</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">require_render</span><span class="p">:</span>
                <span class="n">env</span><span class="p">.</span><span class="nf">render</span><span class="p">()</span>
            
            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_her</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">best_q_action</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">best_q_action</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">goal</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">Return</span> <span class="o">+=</span> <span class="n">reward</span>
            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                
                <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">'</span><span class="s">Return:</span><span class="si">{</span><span class="n">Return</span><span class="si">}</span><span class="sh">'</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="n">env</span><span class="p">.</span><span class="nf">close</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">Return</span>  
</code></pre></div></div> <p>Define the reward and $m$ of <code class="language-plaintext highlighter-rouge">MountainCar-v0</code>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reward_func_MountainCar</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">goal</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    In the original code:
    done = bool(position &gt;= self.goal_position and velocity &gt;= self.goal_velocity)
    </span><span class="sh">'''</span>
    <span class="k">if</span> <span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="n">goal</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">return</span> <span class="mi">0</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="o">-</span><span class="mi">1</span>    

<span class="k">def</span> <span class="nf">m_MountainCar</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="sh">'''</span><span class="s">
    m: S -&gt; G
    for MountainCar environment, s[0] is the position, s[1] is the velocity. The goal is s[0] == 0.5 and s[1] == 0
    </span><span class="sh">'''</span>
    <span class="n">g</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">s</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
    <span class="k">return</span> <span class="n">g</span>
</code></pre></div></div> <p>Run experiment:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">env_name</span> <span class="o">=</span> <span class="sh">'</span><span class="s">MountainCar-v0</span><span class="sh">'</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="nf">make</span><span class="p">(</span><span class="n">env_name</span><span class="p">)</span>

<span class="n">her</span> <span class="o">=</span> <span class="nc">HER</span><span class="p">(</span><span class="n">reward_func</span><span class="o">=</span><span class="n">reward_func_MountainCar</span><span class="p">,</span> <span class="n">m_func</span><span class="o">=</span><span class="n">m_MountainCar</span><span class="p">,</span> <span class="n">sample_strategy</span><span class="o">=</span><span class="sh">'</span><span class="s">final</span><span class="sh">'</span><span class="p">,</span> <span class="n">replay_buffer_capacity</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">k_goals</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">dqn_her</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">state_size</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action_size</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">goal_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">replay_buffer</span><span class="o">=</span><span class="n">her</span><span class="p">,</span> <span class="n">gradient_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="p">.</span><span class="mi">001</span><span class="p">)</span>
<span class="n">dqn</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">state_size</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">observation_space</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">action_size</span><span class="o">=</span><span class="n">env</span><span class="p">.</span><span class="n">action_space</span><span class="p">.</span><span class="n">n</span><span class="p">,</span> <span class="n">gradient_steps</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="p">.</span><span class="mi">001</span><span class="p">)</span>
<span class="n">save_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="sh">'</span><span class="s">drl</span><span class="sh">'</span><span class="p">,</span> <span class="n">dqn_her</span><span class="p">.</span><span class="n">name</span><span class="p">,</span> <span class="n">env_name</span><span class="p">)</span> 

<span class="n">return_path_her</span> <span class="o">=</span> <span class="nf">train</span><span class="p">(</span><span class="n">dqn_her</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">update_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">goal</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([.</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
<span class="n">num_tests</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">Rs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_tests</span><span class="p">):</span>
    <span class="n">Return</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">dqn_her</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">require_render</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">goal</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
    <span class="n">Rs</span> <span class="o">+=</span> <span class="n">Return</span>
<span class="nf">print</span><span class="p">(</span><span class="n">Rs</span> <span class="o">/</span> <span class="n">num_tests</span><span class="p">)</span>

<span class="c1"># In comparison
</span><span class="n">return_path_dqn</span> <span class="o">=</span> <span class="nf">train</span><span class="p">(</span><span class="n">dqn</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_episodes</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">save_path</span><span class="o">=</span><span class="n">save_path</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">update_every</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
<span class="n">num_tests</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">Rs</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_tests</span><span class="p">):</span>
    <span class="n">Return</span> <span class="o">=</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">dqn</span><span class="p">,</span> <span class="n">env</span><span class="p">,</span> <span class="n">num_timesteps</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">require_render</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">Rs</span> <span class="o">+=</span> <span class="n">Return</span>
<span class="nf">print</span><span class="p">(</span><span class="n">Rs</span> <span class="o">/</span> <span class="n">num_tests</span><span class="p">)</span>
</code></pre></div></div> <p>Check my post on Discovery Lab: <a href="https://mp.weixin.qq.com/s/CCDmxhc79WTWAnImsegvhQ" rel="external nofollow noopener" target="_blank">【每周一读】”事后诸葛亮”——一种简单有效的强化学习技术HER（文末附Pytorch代码）</a></p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-more-mcts/">Some Recent Advancement Around MuZero</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-mpc/">MPC with a Differentiable Forward Model: An Implementation with Jax</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/blog-muax/">Adding MuZero into RL Toolkits at Ease</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/blog-tabulardl/">What are the Effective Deep Learning Models for Tabular Data? </a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/blog-drlhft/">Will DRL Make Profit in High-Frequency Trading?</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Bowen Fang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>